{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'H:\\RediMinds\\VCQI'\n",
    "train = pd.read_csv(input_path+\"\\VCQI_clean_train.csv\")\n",
    "test = pd.read_csv(input_path+\"\\VCQI_clean_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(labels='INTRA_OP_COMPLICATIONS', axis = 'columns').copy()\n",
    "y_train = train['INTRA_OP_COMPLICATIONS'].copy()\n",
    "x_test = test.drop(labels='INTRA_OP_COMPLICATIONS', axis = 'columns').copy()\n",
    "y_test = test['INTRA_OP_COMPLICATIONS'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% pos labels train 0.06\n",
      "% pos labels test 0.06\n"
     ]
    }
   ],
   "source": [
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Cataegorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONE HOT CODE data for training\n",
    "\n",
    "# Create dummy variables\n",
    "with open (input_path+'\\cat_col', 'rb') as fp:\n",
    "    cat_col = pickle.load(fp)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "one_hot_encoded_array = encoder.fit_transform(x_train[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_train_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_train = x_train.merge(x_train_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_train = x_train.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "one_hot_encoded_array = encoder.transform(x_test[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_test_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_test = x_test.merge(x_test_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_test = x_test.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in trainset 1183\n",
      "Number records in testset 507\n",
      "% pos labels train 0.06\n",
      "% pos labels test 0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records in trainset {}\".format(len(x_train)))\n",
    "print(\"Number records in testset {}\".format(len(x_test)))\n",
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for logist Classifier\n",
    "numeric_features = x_train.select_dtypes('float').columns.tolist()\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubhav\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "DTC = DecisionTreeClassifier()\n",
    "parameter_dist = {'max_features':['sqrt','log2',None],\n",
    "                  'min_samples_leaf': [2,5,10,15,20], \n",
    "                  'class_weight' :['balanced'], \n",
    "                  'random_state': [1234]}  \n",
    "\n",
    "\n",
    "classifier_DTC = GridSearchCV(DTC, parameter_dist, n_jobs = -1, scoring = 'average_precision', cv = 10 )\n",
    "results_classifier_DTC = classifier_DTC.fit(x_train, y_train)\n",
    "y_DTC = results_classifier_DTC.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = results_classifier_DTC\n",
    "results_DTC = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_DTC['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_DTC['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_features': None, 'min_samples_leaf': 5, 'random_state': 1234}\n",
      "\n",
      " Model Best CV score: \n",
      "0.423976866932126\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.7037224065791372\n",
      "\n",
      " Confusion Matrix : \n",
      "[[442  36]\n",
      " [ 15  14]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.92      0.95       478\n",
      "           1       0.28      0.48      0.35        29\n",
      "\n",
      "    accuracy                           0.90       507\n",
      "   macro avg       0.62      0.70      0.65       507\n",
      "weighted avg       0.93      0.90      0.91       507\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.7154451017169241\n",
      "\n",
      " PR-ROC: \n",
      "0.46101677465015034\n"
     ]
    }
   ],
   "source": [
    "# Decison Tree Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_DTC)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_DTC)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_DTC)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_DTC['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_DTC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_AUC(result):\n",
    "    from sklearn.utils import resample\n",
    "    #from tqdm import tqdm\n",
    "\n",
    "    n_iter = 10000\n",
    "    roc_auc = list()\n",
    "    prc_auc = list()\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result_sample = resample(result, n_samples = len(result),random_state=i)\n",
    "        \n",
    "        #Calculating AUROC for each sample\n",
    "        y_ACTUAL= result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        roc_auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "        #calculate AUPRC for each sample\n",
    "        y_ACTUAL = result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        yhat = result_sample['pred_label']\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        prc_auc.append(metrics.auc(recall,precision))\n",
    "    \n",
    "    return roc_auc, prc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_DTC, pr_auc_DTC = bootstrapped_AUC(results_DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_DTC</th>\n",
       "      <th>pr_auc_DTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.716055</td>\n",
       "      <td>0.461903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.048957</td>\n",
       "      <td>0.092397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.522851</td>\n",
       "      <td>0.107353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.5%</td>\n",
       "      <td>0.619541</td>\n",
       "      <td>0.272551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.715453</td>\n",
       "      <td>0.465068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97.5%</td>\n",
       "      <td>0.813886</td>\n",
       "      <td>0.637792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.927416</td>\n",
       "      <td>0.762742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_DTC    pr_auc_DTC\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.716055      0.461903\n",
       "std        0.048957      0.092397\n",
       "min        0.522851      0.107353\n",
       "2.5%       0.619541      0.272551\n",
       "50%        0.715453      0.465068\n",
       "97.5%      0.813886      0.637792\n",
       "max        0.927416      0.762742"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_DTC': roc_auc_DTC,\n",
    "        'pr_auc_DTC': pr_auc_DTC,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubhav\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "RFC = RandomForestClassifier(random_state=42)\n",
    "parameter_dist = {'n_estimators': [100,200,300,500,1000], \n",
    "                  'max_features':['sqrt','log2',None],\n",
    "                  'min_samples_leaf': [1,2,3,4,5], \n",
    "                  'class_weight' :['balanced'] \n",
    "} \n",
    "\n",
    "\n",
    "classifier_RFR = GridSearchCV(RFC, parameter_dist, n_jobs = -1,cv=10, scoring = 'average_precision', return_train_score=True)\n",
    "results_classifier_RFR = classifier_RFR.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RFR = classifier_RFR.predict(x_test)\n",
    "classifier = classifier_RFR\n",
    "results_RFR = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_RFR['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_RFR['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 500}\n",
      "\n",
      " Model Best CV score: \n",
      "0.5543264094974539\n",
      "\n",
      " Confusion Matrix : \n",
      "[[478   0]\n",
      " [ 23   6]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       478\n",
      "           1       1.00      0.21      0.34        29\n",
      "\n",
      "    accuracy                           0.95       507\n",
      "   macro avg       0.98      0.60      0.66       507\n",
      "weighted avg       0.96      0.95      0.94       507\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.7493146732073294\n",
      "\n",
      " PR-ROC: \n",
      "0.4340967039716731\n",
      "0.43764831216320843\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_RFR)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_RFR)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_RFR['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_RFR)))\n",
    "print(metrics.average_precision_score(y_test, results_RFR['pred_prob']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_RFR, pr_auc_RFR = bootstrapped_AUC(results_RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_RFR</th>\n",
       "      <th>pr_auc_RFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.749886</td>\n",
       "      <td>0.435497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.054096</td>\n",
       "      <td>0.093085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.511153</td>\n",
       "      <td>0.087730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.5%</td>\n",
       "      <td>0.641087</td>\n",
       "      <td>0.247386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.751272</td>\n",
       "      <td>0.437858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97.5%</td>\n",
       "      <td>0.853273</td>\n",
       "      <td>0.613632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.967280</td>\n",
       "      <td>0.758661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_RFR    pr_auc_RFR\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.749886      0.435497\n",
       "std        0.054096      0.093085\n",
       "min        0.511153      0.087730\n",
       "2.5%       0.641087      0.247386\n",
       "50%        0.751272      0.437858\n",
       "97.5%      0.853273      0.613632\n",
       "max        0.967280      0.758661"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_RFR': roc_auc_RFR,\n",
    "        'pr_auc_RFR': pr_auc_RFR,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "model = LogisticRegression()\n",
    "parameter_dist = {'classifier__solver':['saga','liblinear','newton-cg','lbfgs'],\n",
    "                  'classifier__penalty':['l2'],\n",
    "                  'classifier__max_iter':[10000],\n",
    "                  'classifier__class_weight':['balanced'],\n",
    "                  'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      #('pca',PCA()),\n",
    "                      ('classifier', LogisticRegression(random_state = 42))])\n",
    "logit = GridSearchCV(clf,parameter_dist,n_jobs=-1,scoring= 'average_precision', cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubhav\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#train and fit to original data\n",
    "classifier_logit = logit.fit(x_train, y_train)\n",
    "y_logit = classifier_logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_logit\n",
    "results_logit = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_logit['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_logit['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 0.001, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 10000, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "\n",
      " Model Best CV score: \n",
      "0.4333727615692587\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.7485211369210791\n",
      "\n",
      " Confusion Matrix : \n",
      "[[320 158]\n",
      " [  5  24]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.67      0.80       478\n",
      "           1       0.13      0.83      0.23        29\n",
      "\n",
      "    accuracy                           0.68       507\n",
      "   macro avg       0.56      0.75      0.51       507\n",
      "weighted avg       0.94      0.68      0.76       507\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.8152503246284808\n",
      "\n",
      " PR-ROC: \n",
      "0.33103495850561937\n",
      "0.34250710327979905\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_logit)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_logit)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_logit)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_logit['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_logit)))\n",
    "print(metrics.average_precision_score(y_test, results_logit['pred_prob']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_logit, pr_auc_logit = bootstrapped_AUC(results_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_logit</th>\n",
       "      <th>pr_auc_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.815757</td>\n",
       "      <td>0.337890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.041475</td>\n",
       "      <td>0.091023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>0.603788</td>\n",
       "      <td>0.054451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2.5%</td>\n",
       "      <td>0.728490</td>\n",
       "      <td>0.165363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.818007</td>\n",
       "      <td>0.336984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97.5%</td>\n",
       "      <td>0.892863</td>\n",
       "      <td>0.517857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.970575</td>\n",
       "      <td>0.666414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       roc_auc_logit  pr_auc_logit\n",
       "count   10000.000000  10000.000000\n",
       "mean        0.815757      0.337890\n",
       "std         0.041475      0.091023\n",
       "min         0.603788      0.054451\n",
       "2.5%        0.728490      0.165363\n",
       "50%         0.818007      0.336984\n",
       "97.5%       0.892863      0.517857\n",
       "max         0.970575      0.666414"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_logit': roc_auc_logit,\n",
    "        'pr_auc_logit': pr_auc_logit,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anubhav\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    postives_ratio = round(sum(result['true_label'])/(len(result['true_label'])),2)\n",
    "    return recall, precision, f1, prc_auc, postives_ratio\n",
    "\n",
    "fpr_logit, tpr_logit, thresholds_logit, roc_auc_logit = calc_aucroc_data(results_logit)\n",
    "recall_logit, precision_logit, f1_logit, prc_auc_logit, postives_ratio_logit = calc_aucpr_data(results_logit)\n",
    "\n",
    "fpr_DTC, tpr_DTC, thresholds_DTC, roc_auc_DTC = calc_aucroc_data(results_DTC)\n",
    "recall_DTC, precision_DTC, f1_DTC, prc_auc_DTC, postives_ratio_DTC = calc_aucpr_data(results_DTC)\n",
    "\n",
    "fpr_RFR, tpr_RFR, thresholds_RFR, roc_auc_RFR = calc_aucroc_data(results_RFR)\n",
    "recall_RFR, precision_RFR, f1_RFR, prc_auc_RFR, postives_ratio_RFR = calc_aucpr_data(results_RFR)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "ax1.plot(fpr_logit, tpr_logit, label = 'AUC logit = %0.2f' % roc_auc_logit)\n",
    "ax1.plot(fpr_DTC, tpr_DTC, label = 'AUC DTC = %0.2f' % roc_auc_DTC)\n",
    "ax1.plot(fpr_RFR, tpr_RFR, label = 'AUC RFR = %0.2f' % roc_auc_RFR)\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "ax2.plot(recall_logit, precision_logit, label = 'AUC logit = %.2f' % (prc_auc_logit))\n",
    "ax2.plot(recall_DTC, precision_DTC, label = 'AUC DTC = %.2f' % (prc_auc_DTC))\n",
    "ax2.plot(recall_RFR, precision_RFR, label = 'AUC RFR = %.2f' % (prc_auc_RFR))\n",
    "positive_class_ratio = postives_ratio_logit\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/DTC.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Decision Tree Classifier with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_DTC, output_path+'/DTC.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/RFR.joblib']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Random Forest Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_RFR, output_path+'/RFR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/Logit.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export LOgistic Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_logit, output_path+'/Logit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/OHE.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(encoder, output_path+'/OHE.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

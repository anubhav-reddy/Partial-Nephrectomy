{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'H:\\RediMinds\\VCQI'\n",
    "train = pd.read_csv(input_path+\"\\VCQI_clean_train.csv\")\n",
    "test = pd.read_csv(input_path+\"\\VCQI_clean_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(labels='INTRA_OP_COMPLICATIONS', axis = 'columns').copy()\n",
    "y_train = train['INTRA_OP_COMPLICATIONS'].copy()\n",
    "x_test = test.drop(labels='INTRA_OP_COMPLICATIONS', axis = 'columns').copy()\n",
    "y_test = test['INTRA_OP_COMPLICATIONS'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% pos labels train 0.05\n",
      "% pos labels test 0.05\n"
     ]
    }
   ],
   "source": [
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Cataegorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONE HOT CODE data for training\n",
    "\n",
    "# Create dummy variables\n",
    "with open (input_path+'\\cat_col', 'rb') as fp:\n",
    "    cat_col = pickle.load(fp)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "one_hot_encoded_array = encoder.fit_transform(x_train[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_train_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_train = x_train.merge(x_train_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_train = x_train.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "one_hot_encoded_array = encoder.transform(x_test[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_test_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_test = x_test.merge(x_test_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_test = x_test.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in trainset 1985\n",
      "Number records in testset 852\n",
      "% pos labels train 0.05\n",
      "% pos labels test 0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records in trainset {}\".format(len(x_train)))\n",
    "print(\"Number records in testset {}\".format(len(x_test)))\n",
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for logist Classifier\n",
    "numeric_features = x_train.select_dtypes('float').columns.tolist()\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "DTC = DecisionTreeClassifier()\n",
    "parameter_dist = {'max_features':['sqrt','log2',None],\n",
    "                  'min_samples_leaf': [2,5,10,15,20], \n",
    "                  'class_weight' :['balanced'], \n",
    "                  'random_state': [1234]}  \n",
    "\n",
    "\n",
    "classifier_DTC = GridSearchCV(DTC, parameter_dist, n_jobs = -1, scoring = 'roc_auc', cv = 10 )\n",
    "results_classifier_DTC = classifier_DTC.fit(x_train, y_train)\n",
    "y_DTC = results_classifier_DTC.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = results_classifier_DTC\n",
    "results_DTC = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_DTC['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_DTC['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_features': 'log2', 'min_samples_leaf': 20, 'random_state': 1234}\n",
      "\n",
      " Model Best CV score: \n",
      "0.7483350389658336\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.7706349206349206\n",
      "\n",
      " Confusion Matrix : \n",
      "[[612 198]\n",
      " [  9  33]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.76      0.86       810\n",
      "           1       0.14      0.79      0.24        42\n",
      "\n",
      "    accuracy                           0.76       852\n",
      "   macro avg       0.56      0.77      0.55       852\n",
      "weighted avg       0.94      0.76      0.83       852\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.7891240446796002\n",
      "\n",
      " PR-ROC: \n",
      "0.22165641082185278\n"
     ]
    }
   ],
   "source": [
    "# Decison Tree Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_DTC)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_DTC)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_DTC)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_DTC['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_DTC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_AUC(result):\n",
    "    from sklearn.utils import resample\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    n_iter = 10000\n",
    "    roc_auc = list()\n",
    "    prc_auc = list()\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result_sample = resample(result, n_samples = len(result),random_state=i)\n",
    "        \n",
    "        #Calculating AUROC for each sample\n",
    "        y_ACTUAL= result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        roc_auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "        #calculate AUPRC for each sample\n",
    "        y_ACTUAL = result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        yhat = result_sample['pred_label']\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        prc_auc.append(metrics.auc(recall,precision))\n",
    "    \n",
    "    return roc_auc, prc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_DTC, pr_auc_DTC = bootstrapped_AUC(results_DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_DTC</th>\n",
       "      <th>pr_auc_DTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.789829</td>\n",
       "      <td>0.224274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.035915</td>\n",
       "      <td>0.050727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.646599</td>\n",
       "      <td>0.053281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.715241</td>\n",
       "      <td>0.130995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.791191</td>\n",
       "      <td>0.222106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.855457</td>\n",
       "      <td>0.329292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.909803</td>\n",
       "      <td>0.420473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_DTC    pr_auc_DTC\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.789829      0.224274\n",
       "std        0.035915      0.050727\n",
       "min        0.646599      0.053281\n",
       "2.5%       0.715241      0.130995\n",
       "50%        0.791191      0.222106\n",
       "97.5%      0.855457      0.329292\n",
       "max        0.909803      0.420473"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_DTC': roc_auc_DTC,\n",
    "        'pr_auc_DTC': pr_auc_DTC,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "RFC = RandomForestClassifier(random_state=42)\n",
    "parameter_dist = {'n_estimators': [100,200,300,500], \n",
    "                  'max_features':['sqrt','log2',None],\n",
    "                  'min_samples_leaf': [2,5,10,15,20], \n",
    "                  'class_weight' :['balanced'] \n",
    "} \n",
    "\n",
    "\n",
    "classifier_RFR = GridSearchCV(RFC, parameter_dist, n_jobs = -1,cv=10, scoring = 'roc_auc', return_train_score=True)\n",
    "results_classifier_RFR = classifier_RFR.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RFR = classifier_RFR.predict(x_test)\n",
    "classifier = classifier_RFR\n",
    "results_RFR = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_RFR['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_RFR['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_features': 'log2', 'min_samples_leaf': 5, 'n_estimators': 100}\n",
      "\n",
      " Model Best CV score: \n",
      "0.8382501826855758\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.7670194003527337\n",
      "\n",
      " Confusion Matrix : \n",
      "[[799  11]\n",
      " [ 19  23]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       810\n",
      "           1       0.68      0.55      0.61        42\n",
      "\n",
      "    accuracy                           0.96       852\n",
      "   macro avg       0.83      0.77      0.79       852\n",
      "weighted avg       0.96      0.96      0.96       852\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.8716343327454439\n",
      "\n",
      " PR-ROC: \n",
      "0.4656842619980743\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_RFR)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_RFR)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_RFR)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_RFR['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_RFR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_RFR, pr_auc_RFR = bootstrapped_AUC(results_RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_RFR</th>\n",
       "      <th>pr_auc_RFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.871971</td>\n",
       "      <td>0.476964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.028386</td>\n",
       "      <td>0.087636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.753667</td>\n",
       "      <td>0.197876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.813756</td>\n",
       "      <td>0.310539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.873271</td>\n",
       "      <td>0.474460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.924219</td>\n",
       "      <td>0.654589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.965483</td>\n",
       "      <td>0.790613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_RFR    pr_auc_RFR\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.871971      0.476964\n",
       "std        0.028386      0.087636\n",
       "min        0.753667      0.197876\n",
       "2.5%       0.813756      0.310539\n",
       "50%        0.873271      0.474460\n",
       "97.5%      0.924219      0.654589\n",
       "max        0.965483      0.790613"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_RFR': roc_auc_RFR,\n",
    "        'pr_auc_RFR': pr_auc_RFR,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "model = LogisticRegression()\n",
    "parameter_dist = {'classifier__solver':['saga','liblinear','newton-cg','lbfgs'],\n",
    "                  'classifier__penalty':['l2'],\n",
    "                  'classifier__max_iter':[10000],\n",
    "                  'classifier__class_weight':['balanced'],\n",
    "                  'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      #('pca',PCA()),\n",
    "                      ('classifier', LogisticRegression(random_state = 42))])\n",
    "logit = GridSearchCV(clf,parameter_dist,n_jobs=-1,scoring= 'roc_auc', cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and fit to original data\n",
    "classifier_logit = logit.fit(x_train, y_train)\n",
    "y_logit = classifier_logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_logit\n",
    "results_logit = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_logit['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_logit['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 0.001, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 10000, 'classifier__penalty': 'l2', 'classifier__solver': 'saga'}\n",
      "\n",
      " Model Best CV score: \n",
      "0.828020947378898\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.7854497354497354\n",
      "\n",
      " Confusion Matrix : \n",
      "[[636 174]\n",
      " [  9  33]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.79      0.87       810\n",
      "           1       0.16      0.79      0.27        42\n",
      "\n",
      "    accuracy                           0.79       852\n",
      "   macro avg       0.57      0.79      0.57       852\n",
      "weighted avg       0.95      0.79      0.84       852\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.8601410934744268\n",
      "\n",
      " PR-ROC: \n",
      "0.36407669280280297\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_logit)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_logit)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_logit)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_logit['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_logit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_logit, pr_auc_logit = bootstrapped_AUC(results_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_logit</th>\n",
       "      <th>pr_auc_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.860496</td>\n",
       "      <td>0.376274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029924</td>\n",
       "      <td>0.081325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.723030</td>\n",
       "      <td>0.103066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.797010</td>\n",
       "      <td>0.223866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.861936</td>\n",
       "      <td>0.372471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.914827</td>\n",
       "      <td>0.540366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.954875</td>\n",
       "      <td>0.730973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       roc_auc_logit  pr_auc_logit\n",
       "count   10000.000000  10000.000000\n",
       "mean        0.860496      0.376274\n",
       "std         0.029924      0.081325\n",
       "min         0.723030      0.103066\n",
       "2.5%        0.797010      0.223866\n",
       "50%         0.861936      0.372471\n",
       "97.5%       0.914827      0.540366\n",
       "max         0.954875      0.730973"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_logit': roc_auc_logit,\n",
    "        'pr_auc_logit': pr_auc_logit,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    postives_ratio = round(sum(result['true_label'])/(len(result['true_label'])),2)\n",
    "    return recall, precision, f1, prc_auc, postives_ratio\n",
    "\n",
    "fpr_logit, tpr_logit, thresholds_logit, roc_auc_logit = calc_aucroc_data(results_logit)\n",
    "recall_logit, precision_logit, f1_logit, prc_auc_logit, postives_ratio_logit = calc_aucpr_data(results_logit)\n",
    "\n",
    "fpr_DTC, tpr_DTC, thresholds_DTC, roc_auc_DTC = calc_aucroc_data(results_DTC)\n",
    "recall_DTC, precision_DTC, f1_DTC, prc_auc_DTC, postives_ratio_DTC = calc_aucpr_data(results_DTC)\n",
    "\n",
    "fpr_RFR, tpr_RFR, thresholds_RFR, roc_auc_RFR = calc_aucroc_data(results_RFR)\n",
    "recall_RFR, precision_RFR, f1_RFR, prc_auc_RFR, postives_ratio_RFR = calc_aucpr_data(results_RFR)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "ax1.plot(fpr_logit, tpr_logit, label = 'AUC logit = %0.2f' % roc_auc_logit)\n",
    "ax1.plot(fpr_DTC, tpr_DTC, label = 'AUC DTC = %0.2f' % roc_auc_DTC)\n",
    "ax1.plot(fpr_RFR, tpr_RFR, label = 'AUC RFR = %0.2f' % roc_auc_RFR)\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "ax2.plot(recall_logit, precision_logit, label = 'AUC logit = %.2f' % (prc_auc_logit))\n",
    "ax2.plot(recall_DTC, precision_DTC, label = 'AUC DTC = %.2f' % (prc_auc_DTC))\n",
    "ax2.plot(recall_RFR, precision_RFR, label = 'AUC RFR = %.2f' % (prc_auc_RFR))\n",
    "positive_class_ratio = postives_ratio_logit\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/DTC.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Decision Tree Classifier with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_DTC, output_path+'/DTC.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/RFR.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Random Forest Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_RFR, output_path+'/RFR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/Logit.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export LOgistic Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_logit, output_path+'/Logit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/OHE.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(encoder, output_path+'/OHE.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

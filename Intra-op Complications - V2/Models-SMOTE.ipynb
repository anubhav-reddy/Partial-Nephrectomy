{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'H:\\RediMinds\\VCQI'\n",
    "train = pd.read_csv(input_path+\"\\VCQI_clean_train.csv\")\n",
    "test = pd.read_csv(input_path+\"\\VCQI_clean_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'INTRA_OP_COMPLICATIONS'\n",
    "x_train = train.drop(labels= target, axis = 'columns').copy()\n",
    "y_train = train[target].copy()\n",
    "x_test = test.drop(labels= target, axis = 'columns').copy()\n",
    "y_test = test[target].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% pos labels train 0.06\n",
      "% pos labels test 0.06\n"
     ]
    }
   ],
   "source": [
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Cataegorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONE HOT CODE data for training\n",
    "\n",
    "# Create dummy variables\n",
    "with open (input_path+'\\cat_col', 'rb') as fp:\n",
    "    cat_col = pickle.load(fp)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "one_hot_encoded_array = encoder.fit_transform(x_train[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_train_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_train = x_train.merge(x_train_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_train = x_train.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "one_hot_encoded_array = encoder.transform(x_test[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_test_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_test = x_test.merge(x_test_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_test = x_test.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in trainset 1127\n",
      "Number records in testset 484\n",
      "% pos labels train 0.06\n",
      "% pos labels test 0.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records in trainset {}\".format(len(x_train)))\n",
    "print(\"Number records in testset {}\".format(len(x_test)))\n",
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for logist Classifier\n",
    "sm = SMOTE(random_state=2)\n",
    "numeric_features = x_train.select_dtypes('float').columns.tolist()\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "DTC = DecisionTreeClassifier()\n",
    "parameter_dist = {'DTC__max_features':['sqrt','log2',None],\n",
    "                  'DTC__min_samples_leaf': [2,5,10,15,20], \n",
    "                  'DTC__class_weight' :['balanced'], \n",
    "                  'DTC__random_state': [1234],\n",
    "                  'SMOTE__sampling_strategy':[0.25, 0.50, 0.75, 1.0]\n",
    "                 }\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('SMOTE',sm),\n",
    "                      ('DTC', DecisionTreeClassifier())])\n",
    "classifier_DTC = GridSearchCV(clf, parameter_dist, n_jobs = -1, scoring = 'average_precision', cv = 10 )\n",
    "results_classifier_DTC = classifier_DTC.fit(x_train, y_train)\n",
    "y_DTC = results_classifier_DTC.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = results_classifier_DTC\n",
    "results_DTC = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_DTC['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_DTC['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DTC__class_weight': 'balanced', 'DTC__max_features': None, 'DTC__min_samples_leaf': 20, 'DTC__random_state': 1234, 'SMOTE__sampling_strategy': 0.25}\n",
      "\n",
      " Model Best CV score: \n",
      "0.41411929299921774\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.69490137435021\n",
      "\n",
      " Confusion Matrix : \n",
      "[[425  28]\n",
      " [ 17  14]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       453\n",
      "           1       0.33      0.45      0.38        31\n",
      "\n",
      "    accuracy                           0.91       484\n",
      "   macro avg       0.65      0.69      0.67       484\n",
      "weighted avg       0.92      0.91      0.91       484\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.7702057964822333\n",
      "\n",
      " PR-ROC: \n",
      "0.4624856972114325\n"
     ]
    }
   ],
   "source": [
    "# Decison Tree Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_DTC)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_DTC)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_DTC)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_DTC['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_DTC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_AUC(result):\n",
    "    from sklearn.utils import resample\n",
    "\n",
    "    n_iter = 10000\n",
    "    roc_auc = list()\n",
    "    prc_auc = list()\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result_sample = resample(result, n_samples = len(result),random_state=i)\n",
    "        \n",
    "        #Calculating AUROC for each sample\n",
    "        y_ACTUAL= result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        roc_auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "        #calculate AUPRC for each sample\n",
    "        y_ACTUAL = result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        yhat = result_sample['pred_label']\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        prc_auc.append(metrics.auc(recall,precision))\n",
    "    \n",
    "    return roc_auc, prc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_DTC, pr_auc_DTC = bootstrapped_AUC(results_DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_DTC</th>\n",
       "      <th>pr_auc_DTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.785888</td>\n",
       "      <td>0.232982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.038611</td>\n",
       "      <td>0.058776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.642619</td>\n",
       "      <td>0.055904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.706975</td>\n",
       "      <td>0.126511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.786861</td>\n",
       "      <td>0.230650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.858859</td>\n",
       "      <td>0.355408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.903184</td>\n",
       "      <td>0.457816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_DTC    pr_auc_DTC\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.785888      0.232982\n",
       "std        0.038611      0.058776\n",
       "min        0.642619      0.055904\n",
       "2.5%       0.706975      0.126511\n",
       "50%        0.786861      0.230650\n",
       "97.5%      0.858859      0.355408\n",
       "max        0.903184      0.457816"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_DTC': roc_auc_DTC,\n",
    "        'pr_auc_DTC': pr_auc_DTC,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "RFC = RandomForestClassifier(random_state= 42)\n",
    "parameter_dist = {'RFC__n_estimators': [100,200,300,500], \n",
    "                  'RFC__max_features':['sqrt','log2',None],\n",
    "                  'RFC__min_samples_leaf': [2,5,10,15,20], \n",
    "                  'RFC__class_weight' :['balanced'],\n",
    "                  'SMOTE__sampling_strategy':[0.25, 0.50, 0.75, 1.0]\n",
    "} \n",
    "\n",
    "clf = Pipeline(steps=[('SMOTE',sm),\n",
    "                      ('RFC', RandomForestClassifier(random_state= 42))])\n",
    "classifier_RFR = GridSearchCV(clf, parameter_dist, n_jobs = -1,cv=10, scoring = 'average_precision', return_train_score=True)\n",
    "results_classifier_RFR = classifier_RFR.fit(x_train, y_train)\n",
    "y_RFR = results_classifier_RFR.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = results_classifier_RFR\n",
    "results_RFR = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_RFR['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_RFR['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RFC__class_weight': 'balanced', 'RFC__max_features': 'sqrt', 'RFC__min_samples_leaf': 5, 'RFC__n_estimators': 500, 'SMOTE__sampling_strategy': 0.5}\n",
      "\n",
      " Model Best CV score: \n",
      "0.5589915482360979\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.6902371288186284\n",
      "\n",
      " Confusion Matrix : \n",
      "[[450   3]\n",
      " [ 19  12]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98       453\n",
      "           1       0.80      0.39      0.52        31\n",
      "\n",
      "    accuracy                           0.95       484\n",
      "   macro avg       0.88      0.69      0.75       484\n",
      "weighted avg       0.95      0.95      0.95       484\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.8024638609983622\n",
      "\n",
      " PR-ROC: \n",
      "0.45737479349203364\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_RFR)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_RFR)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_RFR)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_RFR['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_RFR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bootstrapped_AUC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-e884b94205ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mroc_auc_RFR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpr_auc_RFR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbootstrapped_AUC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults_RFR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bootstrapped_AUC' is not defined"
     ]
    }
   ],
   "source": [
    "roc_auc_RFR, pr_auc_RFR = bootstrapped_AUC(results_RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'roc_auc_RFR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-d0bfd6caca54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m dict = {'roc_auc_RFR': roc_auc_RFR,\n\u001b[0m\u001b[0;32m      2\u001b[0m         \u001b[1;34m'pr_auc_RFR'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpr_auc_RFR\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m        }\n\u001b[0;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpercentiles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.975\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'roc_auc_RFR' is not defined"
     ]
    }
   ],
   "source": [
    "dict = {'roc_auc_RFR': roc_auc_RFR,\n",
    "        'pr_auc_RFR': pr_auc_RFR,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "model = LogisticRegression()\n",
    "parameter_dist = {'classifier__solver':['saga','liblinear','newton-cg','lbfgs'],\n",
    "                  'classifier__class_weight': ['balanced'] ,\n",
    "                  'classifier__penalty':['l2'],\n",
    "                  'classifier__max_iter':[10000],\n",
    "                  'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'SMOTE__sampling_strategy':[0.25, 0.50, 0.75, 1.0]\n",
    "                 }\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('SMOTE',sm),\n",
    "                      ('classifier', LogisticRegression())])\n",
    "logit = GridSearchCV(clf,parameter_dist,n_jobs=-1,scoring='recall', cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#train and fit to original data\n",
    "classifier_logit = logit.fit(x_train, y_train)\n",
    "y_logit = classifier_logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_logit\n",
    "results_logit = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_logit['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_logit['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SMOTE__sampling_strategy': 0.5, 'classifier__C': 0.001, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 10000, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
      "\n",
      " Model Best CV score: \n",
      "0.7328558752693624\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.7193263547674997\n",
      "\n",
      " Confusion Matrix : \n",
      "[[301 152]\n",
      " [  7  24]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.66      0.79       453\n",
      "           1       0.14      0.77      0.23        31\n",
      "\n",
      "    accuracy                           0.67       484\n",
      "   macro avg       0.56      0.72      0.51       484\n",
      "weighted avg       0.92      0.67      0.76       484\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.8035320088300221\n",
      "\n",
      " PR-ROC: \n",
      "0.4261929768163066\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_logit)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_logit)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_logit)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_logit['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_logit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_logit, pr_auc_logit = bootstrapped_AUC(results_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_logit</th>\n",
       "      <th>pr_auc_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.854278</td>\n",
       "      <td>0.353287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031017</td>\n",
       "      <td>0.079027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.708775</td>\n",
       "      <td>0.093986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.788533</td>\n",
       "      <td>0.207211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.855644</td>\n",
       "      <td>0.349636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.910525</td>\n",
       "      <td>0.516486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.951388</td>\n",
       "      <td>0.683084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       roc_auc_logit  pr_auc_logit\n",
       "count   10000.000000  10000.000000\n",
       "mean        0.854278      0.353287\n",
       "std         0.031017      0.079027\n",
       "min         0.708775      0.093986\n",
       "2.5%        0.788533      0.207211\n",
       "50%         0.855644      0.349636\n",
       "97.5%       0.910525      0.516486\n",
       "max         0.951388      0.683084"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_logit': roc_auc_logit,\n",
    "        'pr_auc_logit': pr_auc_logit,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    postives_ratio = round(sum(result['true_label'])/(len(result['true_label'])),2)\n",
    "    return recall, precision, f1, prc_auc, postives_ratio\n",
    "\n",
    "fpr_logit, tpr_logit, thresholds_logit, roc_auc_logit = calc_aucroc_data(results_logit)\n",
    "recall_logit, precision_logit, f1_logit, prc_auc_logit, postives_ratio_logit = calc_aucpr_data(results_logit)\n",
    "\n",
    "fpr_DTC, tpr_DTC, thresholds_DTC, roc_auc_DTC = calc_aucroc_data(results_DTC)\n",
    "recall_DTC, precision_DTC, f1_DTC, prc_auc_DTC, postives_ratio_DTC = calc_aucpr_data(results_DTC)\n",
    "\n",
    "fpr_RFR, tpr_RFR, thresholds_RFR, roc_auc_RFR = calc_aucroc_data(results_RFR)\n",
    "recall_RFR, precision_RFR, f1_RFR, prc_auc_RFR, postives_ratio_RFR = calc_aucpr_data(results_RFR)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "ax1.plot(fpr_logit, tpr_logit, label = 'AUC logit = %0.2f' % roc_auc_logit)\n",
    "ax1.plot(fpr_DTC, tpr_DTC, label = 'AUC DTC = %0.2f' % roc_auc_DTC)\n",
    "ax1.plot(fpr_RFR, tpr_RFR, label = 'AUC RFR = %0.2f' % roc_auc_RFR)\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "ax2.plot(recall_logit, precision_logit, label = 'AUC logit = %.2f' % (prc_auc_logit))\n",
    "ax2.plot(recall_DTC, precision_DTC, label = 'AUC DTC = %.2f' % (prc_auc_DTC))\n",
    "ax2.plot(recall_RFR, precision_RFR, label = 'AUC RFR = %.2f' % (prc_auc_RFR))\n",
    "positive_class_ratio = postives_ratio_logit\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/SMOTE_DTC.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Decision Tree Classifier with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_DTC, output_path+'/SMOTE_DTC.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/SMOTE_RFR.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Random Forest Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_RFR, output_path+'/SMOTE_RFR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/SMOTE_Logit.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Random Forest Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_logit, output_path+'/SMOTE_Logit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

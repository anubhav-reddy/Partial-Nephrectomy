{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "input_path = 'H:\\RediMinds\\VCQI'\n",
    "#train = pd.read_csv(input_path+\"\\VCQI_clean_train.csv\")\n",
    "test = pd.read_csv(input_path+\"\\VCQI_clean_test_30_day_complications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = train.drop(labels='INTRA_OP_COMPLICATIONS', axis = 'columns').copy()\n",
    "#y_train = train['INTRA_OP_COMPLICATIONS'].copy()\n",
    "x_test = test.drop(labels='30_day_COMPLICATIONS', axis = 'columns').copy()\n",
    "y_test = test['30_day_COMPLICATIONS'].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% pos labels test 0.09\n"
     ]
    }
   ],
   "source": [
    "# check % of pos labels in test set\n",
    "#print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction for the Random Forest Model\n",
    "with open (input_path+'\\cat_col_30_day_complications', 'rb') as fp:\n",
    "    cat_col = pickle.load(fp)\n",
    "\n",
    "model_path = 'output/models/'\n",
    "from joblib import load\n",
    "encoder = load(model_path+'OHE.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "one_hot_encoded_array = encoder.transform(x_test[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_test_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_test = x_test.merge(x_test_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_test = x_test.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'output/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Decision Tree Classifier\n",
    "from joblib import load\n",
    "DTC = load(model_path+'DTC.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest Classifer\n",
    "from joblib import load\n",
    "RFR = load(model_path+'RFR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest Classifer\n",
    "from joblib import load\n",
    "Logit = load(model_path+'Logit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 13:13:06.455117 14772 deprecation.py:323] From H:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py:1792: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# load Neural Network Model\n",
    "nn_model = keras.models.load_model(model_path+'nn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models trained using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import Decision Tree Classifier\n",
    "from joblib import load\n",
    "SMOTE_DTC = load(model_path+'SMOTE_DTC.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest Classifer\n",
    "from joblib import load\n",
    "SMOTE_RFR = load(model_path+'SMOTE_RFR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Random Forest Classifer\n",
    "from joblib import load\n",
    "SMOTE_Logit = load(model_path+'SMOTE_Logit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Neural Network Model\n",
    "SMOTE_nn_model = keras.models.load_model(model_path+'SMOTE_nn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions for the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate prediction for the Decsiion Tree Model\n",
    "results_DTC = pd.DataFrame(DTC.predict(x_test), columns=['pred_label'])\n",
    "results_DTC['pred_prob'] =  pd.DataFrame(DTC.predict_proba(x_test))[1]\n",
    "results_DTC['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction for the Random Forest Model\n",
    "results_RFR = pd.DataFrame(RFR.predict(x_test), columns=['pred_label'])\n",
    "results_RFR['pred_prob'] =  pd.DataFrame(RFR.predict_proba(x_test))[1]\n",
    "results_RFR['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate prediction for the Logitic Regressions Model\n",
    "results_Logit = pd.DataFrame(Logit.predict(x_test), columns=['pred_label'])\n",
    "results_Logit['pred_prob'] =  pd.DataFrame(Logit.predict_proba(x_test))[1]\n",
    "results_Logit['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate prediction for Neural Network Model\n",
    "\n",
    "# import Random Forest Classifer\n",
    "from joblib import load\n",
    "stdc = load(model_path+'nn_stdc.joblib')\n",
    "\n",
    "#To prepare test data we will use test with one-hotcoded variables\n",
    "nn_test = x_test.copy()\n",
    "float_col = test.select_dtypes('float').columns\n",
    "nn_test[float_col] = pd.DataFrame(stdc.transform(nn_test[float_col]),columns=float_col).copy()\n",
    "\n",
    "# Generate prediction for the Neural Network Model\n",
    "results_NN = pd.DataFrame(nn_model.predict(nn_test), columns=['pred_prob'])\n",
    "results_NN['pred_label'] =  results_NN['pred_prob'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "results_NN['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions for Testset - SMOTE Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction for the Decsiion Tree Model\n",
    "results_SMOTE_DTC = pd.DataFrame(SMOTE_DTC.predict(x_test), columns=['pred_label'])\n",
    "results_SMOTE_DTC['pred_prob'] =  pd.DataFrame(SMOTE_DTC.predict_proba(x_test))[1]\n",
    "results_SMOTE_DTC['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction for the Random Forest Model\n",
    "results_SMOTE_RFR = pd.DataFrame(SMOTE_RFR.predict(x_test), columns=['pred_label'])\n",
    "results_SMOTE_RFR['pred_prob'] =  pd.DataFrame(SMOTE_RFR.predict_proba(x_test))[1]\n",
    "results_SMOTE_RFR['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction for the Logistic Regressions Model\n",
    "results_SMOTE_Logit = pd.DataFrame(SMOTE_Logit.predict(x_test), columns=['pred_label'])\n",
    "results_SMOTE_Logit['pred_prob'] =  pd.DataFrame(SMOTE_Logit.predict_proba(x_test))[1]\n",
    "results_SMOTE_Logit['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate prediction for Neural Network Model\n",
    "# Data has been already prepared in correct format above for Neural Network\n",
    "\n",
    "# import Random Forest Classifer\n",
    "from joblib import load\n",
    "SMOTE_nn_stdc = load(model_path+'SMOTE_nn_stdc.joblib')\n",
    "\n",
    "# Generate prediction for the Neural Network Model\n",
    "results_SMOTE_NN = pd.DataFrame(SMOTE_nn_model.predict(nn_test), columns=['pred_prob'])\n",
    "results_SMOTE_NN['pred_label'] =  results_SMOTE_NN['pred_prob'].apply(lambda x: 1 if x>=0.5 else 0)\n",
    "results_SMOTE_NN['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the models for the testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate Precision-Recall Curve\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "\n",
    "print(\"AUC-ROC DTC: {:.3f}\".format(metrics.roc_auc_score(results_DTC['true_label'], results_DTC['pred_prob'])))\n",
    "print(\"AUC-ROC LOGIT: {:.3f}\".format(metrics.roc_auc_score(results_Logit['true_label'], results_Logit['pred_prob'])))\n",
    "print(\"AUC-ROC RFR: {:.3f}\".format(metrics.roc_auc_score(results_RFR['true_label'], results_RFR['pred_prob'])))\n",
    "print(\"AUC-ROC NN: {:.3f}\".format(metrics.roc_auc_score(results_NN['true_label'], results_NN['pred_prob'])))\n",
    "\n",
    "\n",
    "print(\"PR-AUC DTC: {:.3f}\".format(calc_aucpr_data(results_DTC)))\n",
    "print(\"PR-AUC LOGIT: {:.3f}\".format(calc_aucpr_data(results_Logit)))\n",
    "print(\"PR-AUC RFR: {:.3f}\".format(calc_aucpr_data(results_RFR)))\n",
    "print(\"PR-AUC NN: {:.3f}\".format(calc_aucpr_data(results_NN)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    mortality_ratio = round(sum(result['true_label'])/(len(result['true_label'])),2)\n",
    "    return recall, precision, f1, prc_auc, mortality_ratio\n",
    "\n",
    "fpr_DTC, tpr_DTC, thresholds_DTC, roc_auc_DTC = calc_aucroc_data(results_DTC)\n",
    "fpr_Logit, tpr_Logit, thresholds_Logit, roc_auc_Logit = calc_aucroc_data(results_Logit)\n",
    "fpr_RFR, tpr_RFR, thresholds_RFR, roc_auc_RFR = calc_aucroc_data(results_RFR)\n",
    "fpr_NN, tpr_NN, thresholds_NN, roc_auc_NN = calc_aucroc_data(results_NN)\n",
    "\n",
    "recall_DTC, precision_DTC, f1_DTC, prc_auc_DTC, mortality_ratio = calc_aucpr_data(results_DTC)\n",
    "recall_Logit, precision_Logit, f1_Logit, prc_auc_Logit, mortality_ratio = calc_aucpr_data(results_Logit)\n",
    "recall_RFR, precision_RFR, f1_RFR, prc_auc_RFR, mortality_ratio = calc_aucpr_data(results_RFR)\n",
    "recall_NN, precision_NN, f1_NN, prc_auc_NN, mortality_ratio = calc_aucpr_data(results_NN)\n",
    "\n",
    "\n",
    "# Plotting performance of models developed using original data\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "\n",
    "#ax1.plot(fpr_DTC, tpr_DTC, label = 'AUC-ROC DTC = %0.2f' % roc_auc_DTC)\n",
    "ax1.plot(fpr_Logit, tpr_Logit, label = 'AUC-ROC Logit = %0.2f' % roc_auc_Logit)\n",
    "ax1.plot(fpr_RFR, tpr_RFR, label = 'AUC-ROC RFR = %0.2f' % roc_auc_RFR)\n",
    "ax1.plot(fpr_NN, tpr_NN, label = 'AUC-ROC NN = %0.2f' % roc_auc_NN)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "\n",
    "#ax2.plot(recall_DTC, precision_DTC, label = 'PR-AUC DTC=%.2f' % (prc_auc_DTC))\n",
    "ax2.plot(recall_Logit, precision_Logit, label = 'PR-AUC Logit=%.2f' % (prc_auc_Logit))\n",
    "ax2.plot(recall_RFR, precision_RFR, label = 'PR-AUC RFR=%.2f' % (prc_auc_RFR))\n",
    "ax2.plot(recall_NN, precision_NN, label = 'PR-AUC NN=%.2f' % (prc_auc_NN))\n",
    "\n",
    "positive_class_ratio = mortality_ratio\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "#fpr_DTC, tpr_DTC, thresholds_DTC, roc_auc_DTC = calc_aucroc_data(results_SMOTE_DTC)\n",
    "fpr_Logit, tpr_Logit, thresholds_Logit, roc_auc_Logit = calc_aucroc_data(results_SMOTE_Logit)\n",
    "fpr_RFR, tpr_RFR, thresholds_RFR, roc_auc_RFR = calc_aucroc_data(results_SMOTE_RFR)\n",
    "fpr_NN, tpr_NN, thresholds_NN, roc_auc_NN = calc_aucroc_data(results_SMOTE_NN)\n",
    "\n",
    "#recall_DTC, precision_DTC, f1_DTC, prc_auc_DTC, mortality_ratio = calc_aucpr_data(results_SMOTE_DTC)\n",
    "recall_Logit, precision_Logit, f1_Logit, prc_auc_Logit, mortality_ratio = calc_aucpr_data(results_SMOTE_Logit)\n",
    "recall_RFR, precision_RFR, f1_RFR, prc_auc_RFR, mortality_ratio = calc_aucpr_data(results_SMOTE_RFR)\n",
    "recall_NN, precision_NN, f1_NN, prc_auc_NN, mortality_ratio = calc_aucpr_data(results_SMOTE_NN)\n",
    "\n",
    "# Plotting performance of models developed using original data\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "\n",
    "#ax1.plot(fpr_DTC, tpr_DTC, label = 'AUC-ROC SMOTE DTC = %0.2f' % roc_auc_DTC)\n",
    "ax1.plot(fpr_Logit, tpr_Logit, label = 'AUC-ROC SMOTE Logit = %0.2f' % roc_auc_Logit)\n",
    "ax1.plot(fpr_RFR, tpr_RFR, label = 'AUC-ROC SMOTE RFR = %0.2f' % roc_auc_RFR)\n",
    "ax1.plot(fpr_NN, tpr_NN, label = 'AUC-ROC SMOTE NN = %0.2f' % roc_auc_NN)\n",
    "\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "\n",
    "#ax2.plot(recall_DTC, precision_DTC, label = 'PR-AUC SMOTE DTC=%.2f' % (prc_auc_DTC))\n",
    "ax2.plot(recall_Logit, precision_Logit, label = 'PR-AUC SMOTE Logit=%.2f' % (prc_auc_Logit))\n",
    "ax2.plot(recall_RFR, precision_RFR, label = 'PR-AUC SMOTE RFR=%.2f' % (prc_auc_RFR))\n",
    "ax2.plot(recall_NN, precision_NN, label = 'PR-AUC SMOTE NN=%.2f' % (prc_auc_NN))\n",
    "\n",
    "positive_class_ratio = mortality_ratio\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Calibration curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def calib_curve(result):\n",
    "    x, y = calibration_curve(result['true_label'],result['pred_prob'],n_bins = 10, strategy='uniform')\n",
    "    return x, y\n",
    "\n",
    "x_DTC, y_DTC = calib_curve(results_DTC)\n",
    "x_Logit, y_Logit = calib_curve(results_Logit)\n",
    "x_RFR, y_RFR = calib_curve(results_RFR)\n",
    "x_NN, y_NN = calib_curve(results_NN)\n",
    "\n",
    "x_SMOTE_DTC, y_SMOTE_DTC = calib_curve(results_SMOTE_DTC)\n",
    "x_SMOTE_Logit, y_SMOTE_Logit = calib_curve(results_SMOTE_Logit)\n",
    "x_SMOTE_RFR, y_SMOTE_RFR = calib_curve(results_SMOTE_RFR)\n",
    "x_SMOTE_NN, y_SMOTE_NN = calib_curve(results_SMOTE_NN)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "fig.suptitle('Comparision of Calibration Plots (Reliability Curves)', fontsize=20)\n",
    "\n",
    "#ax1.plot(x_DTC, y_DTC, marker = 'o', color = 'tab:blue', label = 'DTC')\n",
    "ax1.plot(x_Logit, y_Logit, marker = 'o', color = 'tab:orange', label = 'LOGIT')\n",
    "ax1.plot(x_RFR, y_RFR, marker = 'o', color = 'tab:red', label = 'RFR')\n",
    "ax1.plot(x_NN, y_NN, marker = 'o', color = 'tab:green', label = 'NN')\n",
    "\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "ax1.set_ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "ax1.set_xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "\n",
    "#ax2.plot(x_SMOTE_DTC, y_SMOTE_DTC, marker = 'o', color = 'tab:blue', label = 'DTC')\n",
    "ax2.plot(x_SMOTE_Logit, y_SMOTE_Logit, marker = 'o', color = 'tab:orange', label = 'LOGIT')\n",
    "ax2.plot(x_SMOTE_RFR, y_SMOTE_RFR, marker = 'o', color = 'tab:red', label = 'RFR')\n",
    "ax2.plot(x_SMOTE_NN, y_SMOTE_NN, marker = 'o', color = 'tab:green', label = 'NN')\n",
    "\n",
    "ax2.plot([0, 1], [0, 1],linestyle='--',color='black', label = 'Perfectly Calibrated')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "ax2.set_ylabel('Fraction of Positives',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Mean Predicted value',fontdict={\"size\":15})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

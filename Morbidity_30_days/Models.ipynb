{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'H:\\RediMinds\\VCQI'\n",
    "train = pd.read_csv(input_path+\"\\VCQI_clean_train_30_day_complications.csv\")\n",
    "test = pd.read_csv(input_path+\"\\VCQI_clean_test_30_day_complications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = '30_day_COMPLICATIONS'\n",
    "x_train = train.drop(labels=target, axis = 'columns').copy()\n",
    "y_train = train[target].copy()\n",
    "x_test = test.drop(labels=target, axis = 'columns').copy()\n",
    "y_test = test[target].copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% pos labels train 0.09\n",
      "% pos labels test 0.09\n"
     ]
    }
   ],
   "source": [
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding Cataegorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ONE HOT CODE data for training\n",
    "\n",
    "# Create dummy variables\n",
    "with open (input_path+'\\cat_col_30_day_complications', 'rb') as fp:\n",
    "    cat_col = pickle.load(fp)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n",
    "\n",
    "one_hot_encoded_array = encoder.fit_transform(x_train[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_train_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_train = x_train.merge(x_train_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_train = x_train.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables\n",
    "one_hot_encoded_array = encoder.transform(x_test[cat_col]).toarray()\n",
    "column_name = encoder.get_feature_names(cat_col)\n",
    "x_test_OHE =  pd.DataFrame(one_hot_encoded_array, columns= column_name)\n",
    "x_test = x_test.merge(x_test_OHE, how = 'left', left_index = True, right_index =True) # create dummy variables\n",
    "x_test = x_test.drop(labels = cat_col, axis = 'columns') # drop original variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in trainset 1985\n",
      "Number records in testset 852\n",
      "% pos labels train 0.09\n",
      "% pos labels test 0.09\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records in trainset {}\".format(len(x_train)))\n",
    "print(\"Number records in testset {}\".format(len(x_test)))\n",
    "print('% pos labels train {:.2f}'.format(y_train.sum()/len(y_train)))\n",
    "print('% pos labels test {:.2f}'.format(y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for logist Classifier\n",
    "numeric_features = x_train.select_dtypes('float').columns.tolist()\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_features)], remainder='passthrough')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "DTC = DecisionTreeClassifier()\n",
    "parameter_dist = {'max_features':['sqrt','log2',None],\n",
    "                  'min_samples_leaf': [2,5,10,15,20], \n",
    "                  'class_weight' :['balanced'], \n",
    "                  'random_state': [1234]}  \n",
    "\n",
    "\n",
    "classifier_DTC = GridSearchCV(DTC, parameter_dist, n_jobs = -1, scoring = 'roc_auc', cv = 10 )\n",
    "results_classifier_DTC = classifier_DTC.fit(x_train, y_train)\n",
    "y_DTC = results_classifier_DTC.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = results_classifier_DTC\n",
    "results_DTC = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_DTC['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_DTC['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_features': None, 'min_samples_leaf': 20, 'random_state': 1234}\n",
      "\n",
      " Model Best CV score: \n",
      "0.760691670575574\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.6124759111855886\n",
      "\n",
      " Confusion Matrix : \n",
      "[[587 188]\n",
      " [ 41  36]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.84       775\n",
      "           1       0.16      0.47      0.24        77\n",
      "\n",
      "    accuracy                           0.73       852\n",
      "   macro avg       0.55      0.61      0.54       852\n",
      "weighted avg       0.86      0.73      0.78       852\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.6632006702974445\n",
      "\n",
      " PR-ROC: \n",
      "0.1949738532593528\n"
     ]
    }
   ],
   "source": [
    "# Decison Tree Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_DTC)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_DTC)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_DTC)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_DTC['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_DTC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_AUC(result):\n",
    "    from sklearn.utils import resample\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    n_iter = 10000\n",
    "    roc_auc = list()\n",
    "    prc_auc = list()\n",
    "\n",
    "\n",
    "    for i in range(n_iter):\n",
    "        result_sample = resample(result, n_samples = len(result),random_state=i)\n",
    "        \n",
    "        #Calculating AUROC for each sample\n",
    "        y_ACTUAL= result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        roc_auc.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "        #calculate AUPRC for each sample\n",
    "        y_ACTUAL = result_sample['true_label']\n",
    "        scores_prob = result_sample['pred_prob']\n",
    "        yhat = result_sample['pred_label']\n",
    "        precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "        prc_auc.append(metrics.auc(recall,precision))\n",
    "    \n",
    "    return roc_auc, prc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_DTC, pr_auc_DTC = bootstrapped_AUC(results_DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_DTC</th>\n",
       "      <th>pr_auc_DTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.663187</td>\n",
       "      <td>0.197549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.031563</td>\n",
       "      <td>0.036774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.546280</td>\n",
       "      <td>0.081531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.599946</td>\n",
       "      <td>0.131814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.663796</td>\n",
       "      <td>0.195541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.724007</td>\n",
       "      <td>0.274207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.794121</td>\n",
       "      <td>0.345552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_DTC    pr_auc_DTC\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.663187      0.197549\n",
       "std        0.031563      0.036774\n",
       "min        0.546280      0.081531\n",
       "2.5%       0.599946      0.131814\n",
       "50%        0.663796      0.195541\n",
       "97.5%      0.724007      0.274207\n",
       "max        0.794121      0.345552"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_DTC': roc_auc_DTC,\n",
    "        'pr_auc_DTC': pr_auc_DTC,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "RFC = RandomForestClassifier(random_state=42)\n",
    "parameter_dist = {'n_estimators': [100,200,300,500], \n",
    "                  'max_features':['sqrt','log2',None],\n",
    "                  'min_samples_leaf': [2,5,10,15,20], \n",
    "                  'class_weight' :['balanced'] \n",
    "} \n",
    "\n",
    "\n",
    "classifier_RFR = GridSearchCV(RFC, parameter_dist, n_jobs = -1,cv=10, scoring = 'roc_auc', return_train_score=True)\n",
    "results_classifier_RFR = classifier_RFR.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RFR = classifier_RFR.predict(x_test)\n",
    "classifier = classifier_RFR\n",
    "results_RFR = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_RFR['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_RFR['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'class_weight': 'balanced', 'max_features': 'sqrt', 'min_samples_leaf': 5, 'n_estimators': 300}\n",
      "\n",
      " Model Best CV score: \n",
      "0.837868037835858\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.6112023460410557\n",
      "\n",
      " Confusion Matrix : \n",
      "[[736  39]\n",
      " [ 56  21]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       775\n",
      "           1       0.35      0.27      0.31        77\n",
      "\n",
      "    accuracy                           0.89       852\n",
      "   macro avg       0.64      0.61      0.62       852\n",
      "weighted avg       0.88      0.89      0.88       852\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.8260578131545873\n",
      "\n",
      " PR-ROC: \n",
      "0.37605619699055143\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_RFR)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_RFR)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_RFR)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_RFR['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_RFR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_RFR, pr_auc_RFR = bootstrapped_AUC(results_RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_RFR</th>\n",
       "      <th>pr_auc_RFR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.826090</td>\n",
       "      <td>0.377596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.051594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.754452</td>\n",
       "      <td>0.168310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.787887</td>\n",
       "      <td>0.274882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.826408</td>\n",
       "      <td>0.378425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.862030</td>\n",
       "      <td>0.475787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.891835</td>\n",
       "      <td>0.565909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        roc_auc_RFR    pr_auc_RFR\n",
       "count  10000.000000  10000.000000\n",
       "mean       0.826090      0.377596\n",
       "std        0.018696      0.051594\n",
       "min        0.754452      0.168310\n",
       "2.5%       0.787887      0.274882\n",
       "50%        0.826408      0.378425\n",
       "97.5%      0.862030      0.475787\n",
       "max        0.891835      0.565909"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_RFR': roc_auc_RFR,\n",
    "        'pr_auc_RFR': pr_auc_RFR,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import math\n",
    "model = LogisticRegression()\n",
    "parameter_dist = {'classifier__solver':['saga','liblinear','newton-cg','lbfgs'],\n",
    "                  'classifier__penalty':['l2'],\n",
    "                  'classifier__max_iter':[10000],\n",
    "                  'classifier__class_weight':['balanced'],\n",
    "                  'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "                 }\n",
    "\n",
    "\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      #('pca',PCA()),\n",
    "                      ('classifier', LogisticRegression(random_state = 42))])\n",
    "logit = GridSearchCV(clf,parameter_dist,n_jobs=-1,scoring= 'roc_auc', cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and fit to original data\n",
    "classifier_logit = logit.fit(x_train, y_train)\n",
    "y_logit = classifier_logit.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classifier_logit\n",
    "results_logit = pd.DataFrame(classifier.predict(x_test), columns=['pred_label'])\n",
    "results_logit['pred_prob'] =  pd.DataFrame(classifier.predict_proba(x_test))[1]\n",
    "results_logit['true_label'] = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier__C': 0.01, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 10000, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n",
      "\n",
      " Model Best CV score: \n",
      "0.8279032502916787\n",
      "\n",
      " Model Balanced Accuracy: \n",
      "0.6950397989107666\n",
      "\n",
      " Confusion Matrix : \n",
      "[[564 211]\n",
      " [ 26  51]]\n",
      "\n",
      " Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.73      0.83       775\n",
      "           1       0.19      0.66      0.30        77\n",
      "\n",
      "    accuracy                           0.72       852\n",
      "   macro avg       0.58      0.70      0.56       852\n",
      "weighted avg       0.89      0.72      0.78       852\n",
      "\n",
      "\n",
      " AUC-ROC: \n",
      "0.7911353162966066\n",
      "\n",
      " PR-ROC: \n",
      "0.2658469422749462\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Score Raw Data\n",
    "print(classifier.best_params_)\n",
    "print(\"\\n Model Best CV score: \\n\" + str(classifier.best_score_))\n",
    "print(\"\\n Model Balanced Accuracy: \\n\" + str(metrics.balanced_accuracy_score(y_test, y_logit)))\n",
    "print(\"\\n Confusion Matrix : \\n\"+str(metrics.confusion_matrix(y_test, y_logit)))\n",
    "print(\"\\n Classification Report: \\n\"+ str(metrics.classification_report(y_test, y_logit)))\n",
    "print(\"\\n AUC-ROC: \\n\"+ str(metrics.roc_auc_score(y_test, results_logit['pred_prob'])))\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    return prc_auc\n",
    "\n",
    "print(\"\\n PR-ROC: \\n\"+ str(calc_aucpr_data(results_logit)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_logit, pr_auc_logit = bootstrapped_AUC(results_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>roc_auc_logit</th>\n",
       "      <th>pr_auc_logit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.790990</td>\n",
       "      <td>0.268434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022139</td>\n",
       "      <td>0.045398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.683048</td>\n",
       "      <td>0.116027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5%</th>\n",
       "      <td>0.746251</td>\n",
       "      <td>0.184028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.791502</td>\n",
       "      <td>0.266591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97.5%</th>\n",
       "      <td>0.832777</td>\n",
       "      <td>0.361292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.881123</td>\n",
       "      <td>0.455546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       roc_auc_logit  pr_auc_logit\n",
       "count   10000.000000  10000.000000\n",
       "mean        0.790990      0.268434\n",
       "std         0.022139      0.045398\n",
       "min         0.683048      0.116027\n",
       "2.5%        0.746251      0.184028\n",
       "50%         0.791502      0.266591\n",
       "97.5%       0.832777      0.361292\n",
       "max         0.881123      0.455546"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'roc_auc_logit': roc_auc_logit,\n",
    "        'pr_auc_logit': pr_auc_logit,\n",
    "       }\n",
    "pd.DataFrame(dict).describe(percentiles=[0.025,0.975])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:59: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n"
     ]
    }
   ],
   "source": [
    "# Plotting AUROC Curve - Calculatae Metrics\n",
    "from sklearn import metrics\n",
    "def calc_aucroc_data(result):\n",
    "    y_ACTUAL= result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return fpr, tpr, thresholds, roc_auc\n",
    "\n",
    "def calc_aucpr_data(result):\n",
    "    y_ACTUAL = result['true_label']\n",
    "    scores_prob = result['pred_prob']\n",
    "    yhat = result['pred_label']\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_ACTUAL, scores_prob, pos_label=1)\n",
    "    prc_auc = metrics.auc(recall,precision)\n",
    "    f1 = metrics.f1_score(y_ACTUAL, yhat)\n",
    "    ap = metrics.average_precision_score(y_ACTUAL, yhat)\n",
    "    postives_ratio = round(sum(result['true_label'])/(len(result['true_label'])),2)\n",
    "    return recall, precision, f1, prc_auc, postives_ratio\n",
    "\n",
    "fpr_logit, tpr_logit, thresholds_logit, roc_auc_logit = calc_aucroc_data(results_logit)\n",
    "recall_logit, precision_logit, f1_logit, prc_auc_logit, postives_ratio_logit = calc_aucpr_data(results_logit)\n",
    "\n",
    "fpr_DTC, tpr_DTC, thresholds_DTC, roc_auc_DTC = calc_aucroc_data(results_DTC)\n",
    "recall_DTC, precision_DTC, f1_DTC, prc_auc_DTC, postives_ratio_DTC = calc_aucpr_data(results_DTC)\n",
    "\n",
    "fpr_RFR, tpr_RFR, thresholds_RFR, roc_auc_RFR = calc_aucroc_data(results_RFR)\n",
    "recall_RFR, precision_RFR, f1_RFR, prc_auc_RFR, postives_ratio_RFR = calc_aucpr_data(results_RFR)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(16,6))\n",
    "\n",
    "# Plotting AUROC Curve - Plot Curve\n",
    "ax1.set_title('Receiver Operating Characteristic',fontdict={\"size\":20})\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_ylabel('True Positive Rate or Sensitivity', fontdict={\"size\":15})\n",
    "ax1.set_xlabel('False Positive Rate or 1-Specificity',fontdict={\"size\":15})\n",
    "ax1.plot(fpr_logit, tpr_logit, label = 'AUC logit = %0.2f' % roc_auc_logit)\n",
    "ax1.plot(fpr_DTC, tpr_DTC, label = 'AUC DTC = %0.2f' % roc_auc_DTC)\n",
    "ax1.plot(fpr_RFR, tpr_RFR, label = 'AUC RFR = %0.2f' % roc_auc_RFR)\n",
    "ax1.plot([0, 1], [0, 1],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax1.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125), fontsize = 12)  \n",
    "\n",
    "# Plotting Precision-Recall Curve - PLot Curve\n",
    "ax2.set_title('Precision Recall Curve',fontdict={\"size\":20})\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.set_ylabel('Precision',fontdict={\"size\":15})\n",
    "ax2.set_xlabel('Recall',fontdict={\"size\":15})\n",
    "ax2.plot(recall_logit, precision_logit, label = 'AUC logit = %.2f' % (prc_auc_logit))\n",
    "ax2.plot(recall_DTC, precision_DTC, label = 'AUC DTC = %.2f' % (prc_auc_DTC))\n",
    "ax2.plot(recall_RFR, precision_RFR, label = 'AUC RFR = %.2f' % (prc_auc_RFR))\n",
    "positive_class_ratio = postives_ratio_logit\n",
    "ax2.plot([0, 1], [positive_class_ratio, positive_class_ratio],linestyle='--',color='red', label = 'No Skill Line')\n",
    "ax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.125),fontsize = 12)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'output/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/DTC.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Decision Tree Classifier with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_DTC, output_path+'/DTC.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/RFR.joblib']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Random Forest Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_RFR, output_path+'/RFR.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/Logit.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export Random Forest Classifer with Pipeline\n",
    "from joblib import dump\n",
    "dump(classifier_logit, output_path+'/Logit.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/models/OHE.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export LOGIT ONE encoder\n",
    "from joblib import dump\n",
    "dump(encoder, output_path+'/OHE.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
